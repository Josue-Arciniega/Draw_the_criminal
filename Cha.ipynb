{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cha.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x_o3eU-5s4as",
        "outputId": "091690f5-d3d1-44f0-94d8-978eb41ae4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kpqNbui0s3MJ",
        "outputId": "cfa03929-9fe1-436f-f339-860637e78d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0bLUfeT5ls-",
        "colab_type": "code",
        "outputId": "4bddc373-54e0-4ccb-c550-3130df037ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#Ruta raiz\n",
        "PATH = \"/content/drive/My Drive/Challe/\"\n",
        "#datos de entrada \n",
        "INPATH = PATH + \"data/\"\n",
        "#datos de Salida\n",
        "OUTPATH = PATH + \"tg_img/\"\n",
        "#datos de Salida\n",
        "ChkPATH = PATH + \"Checkpoints\"\n",
        "\n",
        "imgurls=!ls -1 \"{INPATH}\"\n",
        "\n",
        "n=700\n",
        "train_n = round(n*.8)\n",
        "\n",
        "#Aleatorizar \n",
        "randurls = np.copy(imgurls)\n",
        "\n",
        "#np.random.seed(23) #esto no cuenta para hacerlo realmente aleatorio se usa una seed para que queden igual al tutoriall\n",
        "np.random.shuffle(randurls)\n",
        "\n",
        "#Particionar train/test dataset\n",
        "tr_urls = randurls[:train_n]\n",
        "ts_urls= randurls[train_n:n]\n",
        "print(len(imgurls),len(tr_urls),len(ts_urls))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "952 560 140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n999nkkAT5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "#Cargar Imagenes\n",
        "IMG_WIDTH =256\n",
        "IMG_HEIGHT =256\n",
        "#rezize\n",
        "def resize(inimg,tgimg, height,width):\n",
        "  inimg = tf.image.resize(inimg,[height,width])\n",
        "  tgimg = tf.image.resize(tgimg,[height,width])\n",
        "  \n",
        "  return inimg,tgimg\n",
        "\n",
        "#Normalizar de -1 a 1 la imagen\n",
        "def normalize(inimg,tgimg):\n",
        "  inimg = (inimg/127.5)-1\n",
        "  tgimg = (tgimg/127.5)-1\n",
        "  \n",
        "  return inimg,tgimg\n",
        "\n",
        "#Aumentar datos crop+flip\n",
        "def random_jitter(inimg, tgimg):\n",
        "  inimg,tgimg = resize(inimg,tgimg,286,286) \n",
        "  #apilar\n",
        "  stacked_image = tf.stack([inimg,tgimg], axis=0)\n",
        "  #recortar\n",
        "  cropped_image = tf.image.random_crop(stacked_image, size=[2,IMG_HEIGHT,IMG_WIDTH,3])\n",
        "\n",
        "  inimg,tgimg=cropped_image[0],cropped_image[1]\n",
        "\n",
        "  if random() > .5:\n",
        "    inimg= tf.image.flip_left_right(inimg)\n",
        "    tgimg= tf.image.flip_left_right(tgimg)\n",
        "\n",
        "  return inimg,tgimg\n",
        "\n",
        " #Cargar imagenes \n",
        "def load_image(filename, augment=True):\n",
        "  inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH+filename),channels=3),tf.float32)[...,:3]\n",
        "  tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(OUTPATH+filename)),tf.float32)[...,:3]\n",
        "  inimg, tgimg =resize(inimg,tgimg,IMG_HEIGHT,IMG_WIDTH)\n",
        "  if augment:\n",
        "    #inimg,tgimg = random_jitter(inimg,tgimg)\n",
        "    inimg,tgimg = normalize(inimg,tgimg)\n",
        "  return inimg,tgimg\n",
        "\n",
        "def load_train_img(filename):\n",
        "  return load_image(filename)\n",
        "\n",
        "def load_test_img(filename):\n",
        "  return load_image(filename,False)\n",
        "#avf=load_train_img(randurls[1])\n",
        "#inimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(INPATH+randurls[0]),channels=3),tf.float32)[...,:3]\n",
        "#inimg = tf.cast(tf.io.decode_gif(tf.io.read_file(INPATH+randurls[10])),tf.float32)[...,:3]\n",
        "#tgimg = tf.cast(tf.image.decode_jpeg(tf.io.read_file(OUTPATH+randurls[10])),tf.float32)[...,:3]\n",
        "#print (inimg.shape)\n",
        "#plt.imshow(((load_train_img(randurls[0])[0])+1)/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3w8uErxCoCg",
        "colab_type": "code",
        "outputId": "451232e4-92a3-47be-dc7a-ada6e3ed860c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(tr_urls)\n",
        "train_dataset = train_dataset.map(load_train_img,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.batch(1)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(ts_urls)\n",
        "test_dataset = test_dataset.map(load_test_img,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(1)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function load_train_img at 0x7f136dc8cea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function load_train_img at 0x7f136dc8cea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <function load_test_img at 0x7f136dc8ce18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <function load_test_img at 0x7f136dc8ce18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM0D6mcGbhsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "def downsample(filters, apply_batchnorm=True):\n",
        "  result = Sequential()\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0,0.02)\n",
        "\n",
        "  #agregar capa convolucional\n",
        "  result.add(Conv2D(filters,\n",
        "             kernel_initializer=initializer,\n",
        "             kernel_size=4,\n",
        "             strides=2,\n",
        "             padding=\"same\",\n",
        "             use_bias=not apply_batchnorm))\n",
        "  #capa de BatchNorm\n",
        "  if apply_batchnorm:\n",
        "    result.add(BatchNormalization())\n",
        "\n",
        "  #capa de activacion leaky_ReLU\n",
        "  result.add(LeakyReLU())\n",
        "\n",
        "  return result\n",
        "  #downsample(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeyJitgMcx5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample(filters, apply_dropout=False):\n",
        "  result = Sequential()\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0,0.02)\n",
        "\n",
        "  #agregar capa convolucional\n",
        "  result.add(Conv2DTranspose(filters,\n",
        "                            kernel_initializer=initializer,\n",
        "                            kernel_size=4,\n",
        "                            strides=2,\n",
        "                            padding=\"same\",\n",
        "                            use_bias=False))\n",
        "  #batch norm\n",
        "  result.add(BatchNormalization())\n",
        "  #capa de Dropout\n",
        "  if apply_dropout:\n",
        "    result.add(Dropout(.5))\n",
        "  #capa de activacion ReLU\n",
        "  result.add(ReLU())\n",
        "\n",
        "  return result\n",
        "#upsample(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szJP6ayBi7sX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[None,None,3])\n",
        "  down_stack = [\n",
        "                downsample(64, apply_batchnorm=False),  #(bs,128,128,64)\n",
        "                downsample(128),                        #(bs,64,64,128)\n",
        "                downsample(256),                        #(bs,32,32,256)\n",
        "                downsample(512),                        #(bs,16,16,512)\n",
        "                downsample(512),                        #(bs,8,8,512)\n",
        "                downsample(512),                        #(bs,4,4,512)\n",
        "                downsample(512),                        #(bs,2,2,512)\n",
        "                downsample(512),                         #(bs,1,1,512)\n",
        "  ]\n",
        "  up_stack = [\n",
        "                upsample(512, apply_dropout=True),      #(bs,2,2,   1024)\n",
        "                upsample(512, apply_dropout=True),      #(bs,4,4,   1024)\n",
        "                upsample(512, apply_dropout=True),      #(bs,8,8,   1024)\n",
        "                upsample(512),                          #(bs,16,16, 1024)\n",
        "                upsample(256),                          #(bs,32,32,  512)\n",
        "                upsample(128),                          #(bs,64,64,  256)\n",
        "                upsample(64),                            #(bs,128,128,128)\n",
        "  ]\n",
        "  initializer = tf.random_normal_initializer(0,0.02)\n",
        "  last = Conv2DTranspose(filters=3,\n",
        "                         kernel_size=4,\n",
        "                         strides = 2,\n",
        "                         padding=\"same\",\n",
        "                         kernel_initializer = initializer,\n",
        "                         activation = \"tanh\")\n",
        "  x = inputs\n",
        "  s = []\n",
        "  concat = Concatenate()\n",
        "  for down in down_stack:\n",
        "    x=down(x)\n",
        "    s.append(x)  \n",
        "\n",
        "  s=reversed(s[:-1])\n",
        "  \n",
        "  for up,sk in zip(up_stack,s):\n",
        "    x=up(x)\n",
        "    x=concat([x,sk])\n",
        "  \n",
        "  last=last(x)\n",
        "  return Model(inputs=inputs,outputs=last)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgp5nTtZSS1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  ini = Input(shape=[None, None, 3],name=\"input_img\")\n",
        "  gen = Input(shape=[None, None, 3],name=\"gener_img\")\n",
        "\n",
        "  con=concatenate([ini,gen])\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0,.02)\n",
        "\n",
        "  down1 = downsample(64,apply_batchnorm=False)(con)\n",
        "  down2 = downsample(128)(down1)\n",
        "  down3 = downsample(256)(down2)\n",
        "  down4 = downsample(512)(down3)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(filters=1,\n",
        "                                kernel_size=4,\n",
        "                                strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                padding=\"same\")(down4)\n",
        "\n",
        "  return tf.keras.Model(inputs=[ini,gen], outputs=last)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAEDrLNaUwbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SimD-cnUVCJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "\n",
        "  #Diferencias entre los true por ser real y el detectado por discriminador.\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output),disc_real_output)\n",
        "  \n",
        "  #Diferencias entre los false por ser generado y el detectado por discriminador.\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output),disc_generated_output)\n",
        "  \n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "  \n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOPwiNPJXlB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LAMBDA = 100\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        " \n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output),disc_generated_output)\n",
        "\n",
        "  #mean absolute_error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "  \n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "  return total_gen_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLF-0SRYo6Y",
        "colab_type": "code",
        "outputId": "60a1d97f-1c9f-4ed2-f30a-7319c24565f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "generator_optimizer      = tf.keras.optimizers.Adam(2e-4,beta_1=.5)\n",
        "discriminator_optimizer  = tf.keras.optimizers.Adam(2e-4,beta_1=.5)\n",
        "\n",
        "checkpoint_prefix = os.path.join(ChkPATH,\"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "#checkpoint.restore('/content/drive/My Drive/IANB/Checkpoints/ckpt-9.index')\n",
        "checkpoint.restore(tf.train.latest_checkpoint(ChkPATH))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f131e324470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jULtQRc3Co5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_images(model,test_input,tar,save_filename=False, display_imgs=True):\n",
        "  \n",
        "  prediction = model(test_input,training=True)\n",
        "\n",
        "  if save_filename:\n",
        "    tf.keras.preprocessing.image.save_img(PATH + 'output/' + save_filename + '.jpg', prediction[0,...])\n",
        "  \n",
        "  plt.figure(figsize=(10,10))\n",
        "\n",
        "  display_list = [test_input[0],tar[0],prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  if display_imgs:\n",
        "    for i in range (3):\n",
        "      plt.subplot(1,3,i+1)\n",
        "      plt.title(title[i])\n",
        "      #toamndo los valores en el dominio [0,1] y plot \n",
        "      plt.imshow(display_list[i]*.5 +.5)\n",
        "      plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJc-E8wGIHic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function()\n",
        "def train_step(input_image,target):\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as discr_tape:\n",
        " \n",
        "    output_img = generator(input_image,training=True)\n",
        "\n",
        "    output_gen_discr = discriminator([output_img,input_image],training=True)\n",
        "\n",
        "    output_trg_discr = discriminator([target,input_image],training=True)\n",
        "\n",
        "    discr_loss=discriminator_loss(output_trg_discr,output_gen_discr)\n",
        "\n",
        "    gen_loss = generator_loss(output_gen_discr,output_img,target)\n",
        "\n",
        "    generator_grads = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
        "\n",
        "    discriminator_grads = discr_tape.gradient(discr_loss,discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_grads,generator.trainable_variables))\n",
        "\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_grads,discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md9rAvesFMEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train(dataset,epochs):\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    imgi = 0\n",
        "    for input_image,target in dataset:\n",
        "      print('epoch' + str(epoch) + 'train: ' + str(imgi) + '/' + str(len(tr_urls)))\n",
        "      imgi+=1\n",
        "      train_step(input_image,target)\n",
        "      clear_output(wait=True)\n",
        "    \n",
        "    imgi = 0\n",
        "    for inp,tar in test_dataset.take(5):\n",
        "      generate_images(generator,inp,tar,str(imgi) + '_' + str(epoch),display_imgs=False)\n",
        "      imgi+=1\n",
        "    #Guardar ckpt cada 50 epochs\n",
        "    if((epoch+1)%25) == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ieo0rApEMsD2",
        "colab_type": "code",
        "outputId": "339132e7-bef9-4fe1-9bc1-48eb0f81983c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train(train_dataset,600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch5train: 385/560\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}